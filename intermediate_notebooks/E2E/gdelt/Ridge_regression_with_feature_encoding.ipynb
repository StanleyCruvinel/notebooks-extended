{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Goldstein scale rating of events from their actors and coverage\n",
    "This example workflow demonstrates the use of cuDF, cuML, and dask_cudf for ETL, model training, and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring and Loading the Data\n",
    "The [GDELT](https://www.gdeltproject.org/) project maintains a publically available 50TB database of world events. This workflow uses a sample of the data obtained through [Google BigQuery](https://console.cloud.google.com/marketplace/details/the-gdelt-project/gdelt-2-events)\n",
    "\n",
    "We work with a ~5.7GB sample of the data stored locally. With appropriate Google Cloud credentials, the [gcs python api](https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries) can be used to query the data directly without using a local copy. The GDELT project provides [a codebook](http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf) to help us understand the dataset's schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_cudf\n",
    "import cudf\n",
    "from os.path import join\n",
    "\n",
    "# The location of our local copy\n",
    "DATA_DIR = \"/datasets/gdelt/events/\"\n",
    "\n",
    "# Define the schema for the dataset. Normally dask_cudf can infer these types, however\n",
    "# it does this on a per-file basis. Some of our features are so sparse that all entries \n",
    "# within a file are null, making it impossible to infer the data type\n",
    "dtypes = (\n",
    "    [\"int64\"] * 4\n",
    "    + [\"float64\"]\n",
    "    + [\"str\"] * 20\n",
    "    + [\"int64\"]\n",
    "    + [\"str\"] * 3\n",
    "    + [\"int64\"]\n",
    "    + [\"float64\"]\n",
    "    + [\"int64\"] * 3\n",
    "    + [\"float64\"]\n",
    "    + [\"int64\"]\n",
    "    + [\"str\"] * 4\n",
    "    + [\"float64\"] * 2\n",
    "    + [\"str\"]\n",
    "    + [\"int64\"]\n",
    "    + [\"str\"] * 4\n",
    "    + [\"float64\"] * 2\n",
    "    + [\"str\"]\n",
    "    + [\"int64\"]\n",
    "    + [\"str\"] * 4\n",
    "    + [\"float64\"] * 2\n",
    "    + [\"str\"]\n",
    "    + [\"int64\"]\n",
    "    + [\"str\"]\n",
    ")\n",
    "\n",
    "# The dataset was partitioned into ~2000 csv files. Let's load a subset of those\n",
    "df = dask_cudf.read_csv(join(DATA_DIR, \"00000000189*\"), dtype=dtypes).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8950745 rows\n",
      "Memory footprint: 1631.46 MB\n",
      "Index(['GLOBALEVENTID', 'SQLDATE', 'MonthYear', 'Year', 'FractionDate',\n",
      "       'Actor1Code', 'Actor1Name', 'Actor1CountryCode', 'Actor1KnownGroupCode',\n",
      "       'Actor1EthnicCode', 'Actor1Religion1Code', 'Actor1Religion2Code',\n",
      "       'Actor1Type1Code', 'Actor1Type2Code', 'Actor1Type3Code', 'Actor2Code',\n",
      "       'Actor2Name', 'Actor2CountryCode', 'Actor2KnownGroupCode',\n",
      "       'Actor2EthnicCode', 'Actor2Religion1Code', 'Actor2Religion2Code',\n",
      "       'Actor2Type1Code', 'Actor2Type2Code', 'Actor2Type3Code', 'IsRootEvent',\n",
      "       'EventCode', 'EventBaseCode', 'EventRootCode', 'QuadClass',\n",
      "       'GoldsteinScale', 'NumMentions', 'NumSources', 'NumArticles', 'AvgTone',\n",
      "       'Actor1Geo_Type', 'Actor1Geo_FullName', 'Actor1Geo_CountryCode',\n",
      "       'Actor1Geo_ADM1Code', 'Actor1Geo_ADM2Code', 'Actor1Geo_Lat',\n",
      "       'Actor1Geo_Long', 'Actor1Geo_FeatureID', 'Actor2Geo_Type',\n",
      "       'Actor2Geo_FullName', 'Actor2Geo_CountryCode', 'Actor2Geo_ADM1Code',\n",
      "       'Actor2Geo_ADM2Code', 'Actor2Geo_Lat', 'Actor2Geo_Long',\n",
      "       'Actor2Geo_FeatureID', 'ActionGeo_Type', 'ActionGeo_FullName',\n",
      "       'ActionGeo_CountryCode', 'ActionGeo_ADM1Code', 'ActionGeo_ADM2Code',\n",
      "       'ActionGeo_Lat', 'ActionGeo_Long', 'ActionGeo_FeatureID', 'DATEADDED',\n",
      "       'SOURCEURL'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# What did that give us?\n",
    "print(f\"Found {len(df)} rows\")\n",
    "print(f\"Memory footprint: {df.__sizeof__() / (1024 * 1024):.2f} MB\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "We don't need all or even most of these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rather than listing the columns we don't want, let's list the ones we do\n",
    "keep_cols = (\n",
    "    \"EventCode\",\n",
    "    \"QuadClass\",\n",
    "    \"GoldsteinScale\",\n",
    "    \"NumMentions\",\n",
    "    \"NumSources\",\n",
    "    \"NumArticles\",\n",
    "    \"AvgTone\",\n",
    "    \"Actor1Type1Code\",\n",
    "    \"Actor1Type2Code\",\n",
    "    \"Actor1Type3Code\",\n",
    "    \"Actor2Type1Code\",\n",
    "    \"Actor2Type2Code\",\n",
    "    \"Actor2Type3Code\",\n",
    "    \"ActionGeo_CountryCode\",\n",
    "    \"Actor1Geo_CountryCode\",\n",
    "    \"Actor2Geo_CountryCode\",\n",
    ")\n",
    "# And drop everything else\n",
    "df = df.drop(col for col in df.columns if col not in keep_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor metadata\n",
    "These codes tell us who, what, and where the relevant actors in the event are.\n",
    "\n",
    "If we look at the codebook, we see that a lot of these features are categorical string labels. CuML can transform these into numerical values for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in (\n",
    "    \"Actor1Type1Code\",\n",
    "    \"Actor1Type2Code\",\n",
    "    \"Actor1Type3Code\",\n",
    "    \"Actor2Type1Code\",\n",
    "    \"Actor2Type2Code\",\n",
    "    \"Actor2Type3Code\",\n",
    "    \"ActionGeo_CountryCode\",\n",
    "    \"Actor1Geo_CountryCode\",\n",
    "    \"Actor2Geo_CountryCode\",\n",
    "):\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4896908\n",
      "13    1214102\n",
      "6     339424\n",
      "5     317936\n",
      "8     291539\n",
      "10     273116\n",
      "20     255840\n",
      "22     239862\n",
      "23     214759\n",
      "24     192015\n",
      "[27 more rows]\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Actor1Type1Code\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuadClass\n",
    "An event's `QuadClass` categorizes it as relating to either **verbal cooperation**, **material cooperation**, **verbal conflict**, or **material conflict**\n",
    "\n",
    "The data is still a bit messy. Despite what the codebook says, `QuadClass` has a huge outlier we need to clean up. Let's also one-hot encode it to preserve the orthogonality of the categories\n",
    "\n",
    "*Incidentally, it looks like verbal cooperation was the most common category, which is always nice to see*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    5504571\n",
      "4    1227629\n",
      "3    1178550\n",
      "2    1039992\n",
      "28702269          1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"QuadClass\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cudf import get_dummies\n",
    "\n",
    "# We can fix that by clipping the data to our known range\n",
    "df[\"QuadClass\"] = df[\"QuadClass\"].applymap(lambda i: min(i, 4))\n",
    "df[\"QuadClass\"] = df[\"QuadClass\"].applymap(lambda i: max(i, 1))\n",
    "\n",
    "# Now we one hot encode and drop the old column\n",
    "df = get_dummies(df, columns=[\"QuadClass\"]).drop(\"QuadClass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Actor1Type1Code', 'Actor1Type2Code', 'Actor1Type3Code',\n",
      "       'Actor2Type1Code', 'Actor2Type2Code', 'Actor2Type3Code', 'EventCode',\n",
      "       'GoldsteinScale', 'NumMentions', 'NumSources', 'NumArticles', 'AvgTone',\n",
      "       'Actor1Geo_CountryCode', 'Actor2Geo_CountryCode',\n",
      "       'ActionGeo_CountryCode', 'QuadClass_1', 'QuadClass_2', 'QuadClass_3',\n",
      "       'QuadClass_4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "- `NumArticles`, `NumSources`, and `NumMentions` tell us the number of source documents, information sources, and mentions across sources that an event recieved within 15 minutes of being discovered by the database. We can use this as a proxy for the importance or impact of an event\n",
    "- `AvgTone` describes the tone of the above documents towards the event, on a scale from -100 (extremely negative) to 100 (extremely positive)\n",
    "- `GoldsteinScale` is our target variable. It serves as a way of quantifying the intensity of positive and negative events.\n",
    "\n",
    "It looks like these variables are filled with `null` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumArticles: 2\n",
      "NumSources: 2\n",
      "NumMentions: 2\n",
      "AvgTone: 2\n",
      "GoldsteinScale: 42\n"
     ]
    }
   ],
   "source": [
    "for col in (\"NumArticles\", \"NumSources\", \"NumMentions\", \"AvgTone\", \"GoldsteinScale\"):\n",
    "    print(f\"{col}: {df[col].isna().value_counts()[True]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportionally there aren't that many `null`s, so we can reasonably replace them with the average feature values. While we're here, let's also remove any outliers from our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"AvgTone\"] = df[\"AvgTone\"].fillna(df[\"AvgTone\"].mean())\n",
    "\n",
    "df[\"GoldsteinScale\"] = df[\"GoldsteinScale\"].applymap(lambda i: min(i, 10))\n",
    "df[\"GoldsteinScale\"] = df[\"GoldsteinScale\"].applymap(lambda i: max(i, -10))\n",
    "df[\"GoldsteinScale\"] = df[\"GoldsteinScale\"].fillna(df[\"GoldsteinScale\"].mean())\n",
    "\n",
    "df[\"NumMentions\"] = (\n",
    "    df[\"NumMentions\"].astype(\"float64\").fillna(df[\"NumMentions\"].mean())\n",
    ")\n",
    "df[\"NumSources\"] = (\n",
    "    df[\"NumSources\"].astype(\"float64\").fillna(df[\"NumSources\"].mean())\n",
    ")\n",
    "df[\"NumArticles\"] = (\n",
    "    df[\"NumArticles\"].astype(\"float64\").fillna(df[\"NumArticles\"].mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to min/max scale `NumSources`, `NumArticles`, and `NumMentions` to maintain consistency with our other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in (\n",
    "    \"NumMentions\",\n",
    "    \"NumSources\",\n",
    "    \"NumArticles\",\n",
    "):\n",
    "    col_max = df[col].max()\n",
    "    col_min = df[col].min()\n",
    "    if col_max != col_min:\n",
    "        df[col] = (df[col] - df[col].mean()) / (col_max - col_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we give cuML our data, we need to ensure all columns have the same dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in filter(lambda col: df[col].dtype != \"float64\", df):\n",
    "    df[col] = df[col].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "Now we can run and score our regression. To do this we'll have to\n",
    "\n",
    "- define our performance metrics\n",
    "- split our data into train and test sets\n",
    "- train and test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As our performance metrics we'll use a basic mean squared error and coefficient of determination implementation\n",
    "def mse(y_test, y_pred):\n",
    "    return ((y_test - y_pred) ** 2).mean()\n",
    "\n",
    "def cod(y_test, y_pred):\n",
    "    y_bar = y_test.mean()\n",
    "    total = ((y_test - y_bar) ** 2).sum()\n",
    "    residuals = ((y_test - y_pred) ** 2).sum()\n",
    "    return 1 - (residuals / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of cuML we can abstract away most of the train/test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "from cuml.linear_model.ridge import Ridge\n",
    "\n",
    "def train_and_score(data, clf, train_frac=0.8, n_runs=20):\n",
    "    mse_scores, cod_scores = [], []\n",
    "    for _ in range(n_runs):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data, y=\"GoldsteinScale\", train_size=train_frac\n",
    "        )\n",
    "        y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "        mse_scores.append(mse(y_test, y_pred))\n",
    "        cod_scores.append(cod(y_test, y_pred))\n",
    "    return mse_scores, cod_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Let's see how our model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median MSE (20 runs): 5.036132676165211\n",
      "median COD (20 runs): 0.7674086552283645\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_runs = 20\n",
    "clf = Ridge()\n",
    "mse_scores, cod_scores = train_and_score(df, clf, n_runs=n_runs)\n",
    "print(f\"median MSE ({n_runs} runs): {np.median(mse_scores)}\")\n",
    "print(f\"median COD ({n_runs} runs): {np.median(cod_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasonably good correlation, though the MSE leaves a bit to be desired. Predicting a value on the Goldstein scale within ~2.24 points is nice, until you remember our values are in \\[-10, 10\\]. Let's try some more intelligent analysis. For instance, what does the learning curve for this model look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance: 0.00019863536617650075\n"
     ]
    }
   ],
   "source": [
    "clf = Ridge()\n",
    "n_runs = 10\n",
    "train_sizes = np.arange(0.0001, 0.91, 0.05)\n",
    "scores = [np.median(train_and_score(df, clf, train_frac=s, n_runs=n_runs)[0])\n",
    "          for s in train_sizes]\n",
    "print(f\"Variance: {np.var(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_sizes, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, our loss changes very little when given 10% of the data or 90% of the data. Were we to deploy this model, satisfied with it's performance, we could comfortably train it on only ~890,000 instances (for an even greater speedup) with minimal performance loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "On our 5.7GB sample of the dataset which consisted of ~8.9 million rows, this notebook takes 2.5 minutes to run from start to finish. Recall that this includes\n",
    "- ETL\n",
    "- Preprocessing and feature engineering, with feature encoding and rescaling\n",
    "- Training and inference for a total of 190 ridge regressors\n",
    "\n",
    "In the same amount of time, the equivalent pandas/scikit-learn implementation barely finishes its etl phase.\n",
    "\n",
    "# The key takeaways\n",
    "With cuDF, cuML, and dask_cudf, we're able to \n",
    "- explore large datasets\n",
    "- quickly analyze and transform our data\n",
    "- train and evaluate models\n",
    "\n",
    "with minimal changes to our existing pandas, scikit-learn, and dask powered workflows, and see drastic improvements in performance, making it easier to build scalable data science pipelines without sacrificing performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
